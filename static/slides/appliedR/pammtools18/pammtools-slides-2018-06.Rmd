---
background-color: whitesmoke
title: "pammtools: Piece-wise exponential Additive Mixed Modeling tools"
author: ["Andreas Bender, Fabian Scheipl",
"Department of Statistics, LMU Munich"]
date: "2018/06/20"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
    css: ["default", "metropolis", "metropolis-fonts", "custom.css"]
---
# Outline
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/github.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">

<br>
<br>
<br>
.font150[
- Introduction

- Time-varying Effects

- Cumulative effects

]

???
$$\newcommand{\ra}{\rightarrow}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\tn}[1]{\textnormal{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\nn}{\nonumber}
\newcommand{\ub}{\underbrace}
\newcommand{\tbf}[1]{\textbf{#1}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bsbeta}{\boldsymbol{\beta}}
\newcommand{\bsgamma}{\boldsymbol{\gamma}}
\newcommand{\bslambda}{\boldsymbol{\lambda}}
\newcommand{\bfS}{\mathbf{S}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfZ}{\mathbf{Z}}
\newcommand{\tz}{t_z}
\newcommand{\tlag}{t_{\text{lag}}}
\newcommand{\tlead}{t_{\text{lead}}}
\newcommand{\tw}{\mathcal{T}_e(t)}
\newcommand{\Tw}[1]{\mathcal{T}^{#1}}
\newcommand{\tilt}{\tilde{t}}
\newcommand{\Zi}{\mathcal{Z}_i(t)}
\newcommand{\CI}{C1}
\newcommand{\CII}{C2}
\newcommand{\CIII}{C3}
\newcommand{\gCII}{g_{_{\CII}}}
\newcommand{\gCIII}{g_{_{\CIII}}}
\newcommand{\gammaEst}{\hat{\gamma}_g^r}
\newcommand{\hatEj}{\hat{e}_{j, r}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\rpexp}{\operatorname{rpexp}}
\newcommand{\Rlang}{\textbf{\textsf{R}}}
\newcommand{\code}[1]{{\small \texttt{#1}}}$$

```{r rsetup, echo=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(
  fig.align  = "center",
  message    = FALSE,
  out.height = "300px",
  dpi        = 1200,
  cache      = TRUE)
options(htmltools.dir.version = FALSE)
library(RefManageR)
BibOptions(check.entries = FALSE, bib.style = "authoryear", style = "markdown",
           dashed = TRUE)
bib <- ReadBib("Remote.bib")

library(mgcv)
library(survival)
library(pammtools)
library(purrr)
library(tidyr)
library(dplyr)
library(ggplot2)
theme_set(theme_bw())
library(patchwork)
Set1 <- RColorBrewer::brewer.pal(9, "Set1")
```


---
# Introduction
- It can be shown that under certain conditions (and after appropriate data preparation)
the Cox model `r Citep(bib, "Cox1972")` can be estimated using Poisson Regression

- Such models are called *Piece-wise Exponential Models* (PEM);
`r Citet(bib, "Whitehead1980", "Friedman1982") `

- PEMs have some disadvantages (compared to Cox regression), but can be improved
by switching to *Piece-wise exponential Additive (Mixed) Models* (PA(M)Ms),
where the baseline hazard (and time-varying effects) are estimated by penalized
splines

- Essentially, this allows the usage of methods and inference procedures
developed for Generalized Additive (Mixed) Models (GAM(M)s) in the context
of time-to-event data analysis (e.g., use `mgcv::gam` or `mboost` for estimation)

- PAMMs are powerful, but data pre-processing and model evaluation can be
cumbersome due to data structure (and lack of readily available tools/software)

- The **`pammtools`** package provides utility functions that facilitate
working with PAMMs

---
# Introduction

- The `tumor` data from the **`pammtools`** package contains:
  - right-censored time-until death (`days`)
  - status indicator (`status`, 0 = censored, 1 = died)
  - covariates: `charlson_score` (Comorbidities),
  `age`, `sex`, `complications`, `metastases`, ...

```{r tumor-data-intro, echo=FALSE}
library(pammtools)
DT::datatable(tumor[, c("days", "status", "charlson_score", "age", "sex", "complications", "metastases")],
  width = 600, options = list(autoWidth=FALSE, pageLength=4))
```


---
# Introduction
- Thus, such time-to-event data can be summarized by
  - tuples $(t, \delta)$, where
      - $t$ (`days`) indicates the time until the event (or censoring/truncation)
      - $\delta$ (`status`) the status at time $t$ (often $\delta \in \{0, 1\}$)
  - covariates $x$ (e.g., `age`)

- "Standard" time-to-event analysis models the *hazard rate* <br>
  $$\lambda(t; x) = \lambda_0(t)\exp(\beta \cdot x)$$

  $\ra \exp(\beta)$: Multiplicative increase of the hazard for 1 unit increase of $x$ (c.p.)

- Other quantities can be derived from the hazard rate
  - Cumulative Hazard: $\Lambda(t;x) = \int_{0}^{t} \lambda(s;x)\mathrm{d}s$
  - Survival Probability: $S(t;x) = \exp(-\Lambda(t;x)) = 1 - F(t)$

---
# Introduction
Analysis of such data is complicated because

- $t$ cannot be observed fully (left-truncation, left-censoring, right-censoring, ...)

- competing events (other than the one of interest) can occur (recurrently)
    during the observation period (Competing Risks, Multistate Models)

- it takes time to observe $t$, consequently:

  - the effect of a covariate (observed at $t=0$) can change over time <br>
    $\ra$ *time-varying effects* (TVE): $\lambda(t;x) = \lambda_0(t)\exp(\beta(t)\cdot x)$

  - covariates can change over time <br>
    $\ra$ *time-dependent covariates* (TDC): $\lambda(t;\bfz) = \lambda_0(t)\exp(\beta \cdot z(t))$

  - combination of both <br>
  $\ra$ *concurrent effects*: $\lambda(t;x) = \lambda_0(t)\exp(\beta(t) * z(t))$ <br>
  $\ra$ *cumulative effects* (later)

---
# Time-varying effect (TVE)
- The effect of a covariate observed (only) at the beginning of the follow-up $(t=0)$
changes over the course of the follow-up:
$$\lambda(t; x) = \lambda_0(t)\exp(\beta(t)\cdot x)$$

- Often, this is the case for *scores* that describe a patient's health status
  at time of admission to the intensive care unit (ICU)/hospital
  - highly indicative of survival at the beginning of the follow-up

  - During the follow-up, the initial health status changes (due to treatment),
  thus the effect tends to go to zero over time (unless score describes conditions
  that worsen over time/can not be improved by treatment)

<!--   - Usually, one assumes that $\beta(t)$ is a smooth
  function, thus we denote TVEs by $f(t)$ -->

  - Example: `veteran` data (see [time-varying effects vignette](https://adibender.github.io/pammtools/articles/tveffects.html) for details and code) <br/>
  $\ra$ Effect of the
  <a href="image/p_karno_tv2.jpeg" align="middle"> Karnofsky performance score over time</a>


---
# Time-dependent covariates (TDC)

- The covariate is observed not only at the beginning of the follow-up $(t=0)$
but also at different time-points during the follow-up:
.font80[
$$\lambda(t; \bfz) = \lambda_0(t)\exp(\beta\cdot z(t))$$
]
- Biomarkers, continuously updated clinical scores, ...

- often difficult to model:
  - updates irregular/different for different subjects (irregular grid)
  - no information on covariate values between updates

- Example `recidivism` data `r Citep(bib, "Rossi1980", "Fox2011")`
  - 432 subjects released from prison where followed for 52 weeks
  - Each week the subjects' employment status was recorded
  $z(t)\in \{0,1\}, t = 1,\ldots,52$
  - See [vignette on time-dependent covariates](https://adibender.github.io/pammtools/articles/tdcovar.html) for details

---
# Cumulative Effects

- Let $z(\tz)$ the value of a time-dependent covariate observed at
exposure time point $\tz$
- e.g., $z(\tz=1)=0.2, z(\tz=2) = 2.7, z(\tz=3) = 1.3, \ldots$
- The hazard at time $t=3$ with a simple cumulative effect could be something like

.font90[
$$\lambda(t=3;\bfz) = \lambda_0(t=3)\exp(\beta_1 z(\tz=1) + \beta_2 z(\tz=2) + \beta_3z(\tz=3))$$
]

- However, usually we would expect that past observations are weighted by
a smooth function (that decreases with latency):

.font90[
$$\lambda(t; \bfz) = \lambda_0(t)\exp\left(\sum_{\tz \leq t} h(t-\tz)z(\tz)\right)$$

- the *partial effect* $h(t-\tz)$ can again be estimated from the data via
penalized splines

]

---
# The Piece-wise Exponential Model (PEM)

The idea of the PEM is to

- divide the follow-up in $J$ intervals $(\kappa_{j-1}, \kappa_{j}], j=1,\ldots,J$

- create pseudo observations *pseudo-observations*
$$\delta_{ij} =
  \begin{cases}
    1, &\text{ if } t_i \in (\kappa_{j-1}, \kappa_j] \wedge \delta_i = 1\\
    0, &\text{ else }
  \end{cases}$$
and offsets $o_{ij} = \log(t_{ij})$,
  where $t_{ij} = \min(\kappa_j-\kappa_{j-1}, t_i-\kappa_{j-1})$

- estimate piece-wise constant hazards in each interval $(\kappa_{j-1}, \kappa_j]$

- Survival times with (piece-wise) constant hazards have (piece-wise) exponential
distribution

---
# The PEM: Example
```{r, ex-wb1, echo = FALSE, fig.width=6, fig.height=3}
# Weibull hazard function
hweibull <- function(t, alpha, lambda) {
  dweibull(t, alpha, lambda)/pweibull(t, alpha, lambda, lower.tail=FALSE)
}

# plot hazard and survival probability:
alpha <- 1.5
lambda <- 10
t <- seq(0, 10, by=.01)
wb_df <- data.frame(t = t) %>%
  mutate(
    hazard   = hweibull(t, alpha, lambda),
    survival = pweibull(t, alpha, lambda, lower.tail = FALSE))
p_wb_hazard <- ggplot(wb_df, aes(x=t, y=hazard)) +
  geom_line(col="red") + ylab(expression(lambda(t))) + xlab("time") +
  geom_vline(xintercept = seq(0, 10, by = 1), lty = 3)
p_wb_surv <- p_wb_hazard + aes(y = survival) + ylab(expression(S(t))) + ylim(c(0,1))
```

- Consider survival times following a Weibull distribution $T \sim WB(\alpha = 1.5, \lambda = 10)$

- The follow-up is partitioned in 10 Intervals $(0,1], (1,2], \ldots, (9,10]$

```{r fig-ex-wb1, dependson="ex-wb1", echo = FALSE, fig.width=6, fig.height=3}
p_wb_hazard + p_wb_surv
```

---
# PEM Example II
- Transform the data according to interval split points

.left-column[
&nbsp;&nbsp; standard format

```{r, ex-wb-sub, echo=FALSE, results="asis"}
wb_sub <- data.frame(id = 1:3, time = c(0.37, 2.1, 1.3), status = c(1, 0, 1))
knitr::kable(wb_sub, format="html")
```
]

.right-column[
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Piece-wise Exponential Data (PED) format

```{r, echo=FALSE, results="asis", dependson="ex-wb-sub"}
ped_sub <- as_ped(wb_sub, Surv(time, status)~., cut = seq(0, 10, by = 1))
knitr::kable(ped_sub, format = "html")
```

```{r eval=FALSE}
glm(ped_status ~ interval, family = poisson(), offset=offset)
```




---
# PEM Example III
```{r, sim-wb, echo=FALSE, dependson="ex-wb1"}
# set number of simulated survival times
n <- 1000
# set seed for RNG
set.seed(24032018)
sim_df_weibull <- data.frame(
  time = rweibull(n, 1.5, 10),
  status = 1)

ped <- as_ped(Surv(time, status)~., cut=seq(0, 10, by = 1), data=sim_df_weibull)
pem <- glm(ped_status ~ interval, data = ped, family=poisson(), offset=offset)
pem_haz_df <- int_info(ped) %>%
  mutate(
    hazard = predict(object=pem, ., type="response"),
    survival = exp(-cumsum(hazard * intlen)))
```

Even if approximation of the hazard is relatively crude, the approximation
of the cumulative hazard (piece-wise linear) and the survival probability
(piece-wise exponential) are usually good:

```{r ex-pem-1, fig.width=6, fig.height=3, echo=FALSE, dependson=c("ex-wb1", "sim-wb")}
p_pem_base <- ggplot(pem_haz_df, aes(x = tend)) + xlab("time")
p_pem_haz <- p_pem_base + aes(y = hazard) +
  geom_line(data=data.frame(tend = t, hazard=hweibull(t, 1.5, 10)), col=2) +
  geom_stephazard() + ylab(expression(lambda(t)))
p_pem_surv <- p_pem_base + aes(y = survival) +
  geom_line(data = data.frame(tend = t, survival=pweibull(t, 1.5, 10, lower.tail=FALSE)), col = 2) +
  geom_surv() +
  ylim(c(0,1)) + ylab(expression(S(t)))
p_pem_haz + geom_vline(xintercept = seq(0, 10, by = 1), lty = 3) +
  p_pem_surv+ geom_vline(xintercept = seq(0, 10, by = 1), lty = 3)
```


---
# PEM Example IV
- (In theory) approximation of the hazard function becomes better with increasing
number of intervals (and $n\ra \infty$)

- In practice (finite $n$):
    - interval-specific hazard estimates become unstable
    - number of parameters increases
    - results sensitive to the placement and frequency of follow-up split points

```{r, est-pem, echo=FALSE, fig.width = 6, fig.height=3, dependson=c("sim-wb", "ex-pem-1")}
ped2 <- as_ped(Surv(time, status)~., cut = seq(0,10, by=.1), data=sim_df_weibull)
pem2 <- glm(ped_status ~ interval, data = ped2, family=poisson(), offset=offset)
pem_haz_df2 <- int_info(ped2) %>%
  mutate(
    hazard = predict(object=pem2, ., type="response"),
    survival = exp(-cumsum(hazard * intlen)))
```

```{r, echo=FALSE, fig.width = 5, fig.height=2.5, dependson=c("sim-wb", "ex-pem-1")}
p_pem_haz %+% pem_haz_df2 + p_pem_surv %+% pem_haz_df2
```

---
# PEM Example V
Solution: Use many follow-up splits and estimate via penalized splines

```{r est-pam, echo=FALSE, dependson="est-pem"}
pam2 <- gam(ped_status ~ s(tend, k = 10), data = ped2, family = poisson(),
  offset = offset, method = "REML")
```

```{r, eval=FALSE}
# PEM
glm(ped_status ~ interval, data = ..., family = poisson(), ...)
# PAM
gam(ped_status ~ s(tend, k = 10), data = ..., family = poisson(), ...)
```

<a href = "pammtools-slides-2018-06_files/figure-html/ex-pem-vs-pam-1.png">

```{r, ex-pem-vs-pam, fig.width=5, fig.height=5, echo=FALSE, dependson="est-pam", out.height="375px"}
pam_haz_df <- int_info(ped2) %>%
  add_hazard(pam2) %>%
  add_surv_prob(pam2)
p_pam_haz <- ggplot(pam_haz_df, aes(x = tend, y = hazard)) +
  geom_line(data=data.frame(tend = t, hazard=hweibull(t, 1.5, 10)), col=2) +
  geom_stephazard() +
  geom_stepribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.2) +
  ylab(expression(lambda(t))) + ggtitle("PAM")
p_pam_surv <-  ggplot(pam_haz_df, aes(x = tend, y = surv_prob)) +
  geom_line(data=data.frame(tend = t, surv_prob=pweibull(t, 1.5, 10, lower.tail=FALSE)), col=2) +
  geom_surv() +
  geom_ribbon(aes(ymin = surv_lower, ymax = surv_upper), alpha = 0.2) +
  ylab(expression(S(t))) + ylim(c(0,1))
p_pem_haz %+% pem_haz_df2 + ggtitle("PEM") + p_pam_haz + p_pem_surv %+% pem_haz_df2 + p_pam_surv +
  plot_layout(ncol =2, byrow = F)
```
</a>



---
# Veteran data
```{r veteran-data}
data("veteran", package="survival")
## transform to PED format
ped_veteran <- as_ped(Surv(time, status)~., data = veteran, max_time = 400)
ped_veteran %>% filter(id == 1) %>% slice(c(1:4,(n()-3):n())) %>% select(1:8)
```

---
# PEM vs. Cox
- PEM and Cox PH are equivalent (w.r.t. to estimation of $\bsbeta$) if
  - there are no ties in the data
  - unique event and censoring times are used as split-points

```{r}
veteran2 <-veteran %>% group_by(time) %>% slice(1) # exclude ties (for illustration)
# Cox model
cox <- coxph(Surv(time, status)~ age + karno + trt, data=veteran2)
# Transform Data to PED format
ped_vet <- as_ped(veteran2, Surv(time, status)~., cut=veteran2$time)
# Fit PEM
pem <- glm(ped_status ~ interval + age + karno + trt,
  data=ped_vet, family = poisson(), offset = offset)
# Compare coefficient estimates
cbind(coef(pem)[102:104], coef(cox))
```

---
# PEM vs. PAM
- PAM only needs to estimate $k=10 < J$ parameters
- Estimates of neighboring hazards more similar due to penalization
- Can be feasibly extended to include multiple time-varying effects (later)

```{r}
# Transform Data to PED format
ped_vet <- as_ped(veteran, Surv(time, status)~.)
# Fit Cox
cox <- coxph(Surv(time, status)~ age + karno + trt, data=veteran)
# Fit PEM
pem <- glm(ped_status ~ interval +  age + karno + trt, data=ped_vet,
  family = poisson(), offset = offset)
pam <- gam(ped_status ~ s(tend, k=10) +  age + karno + trt, data=ped_vet,
  family = poisson(), offset = offset)
# Compare coefficient estimates
cbind(coef(pem)[98:100], coef(pam)[2:4], coef(cox))
```


---
# TVE Karnofsky Score
```{r, echo=FALSE}
data("veteran", package="survival")
veteran <- veteran %>%
  mutate(
    trt   = 1 * (trt == 2),
    prior = 1 * (prior == 10))
ped_vet <- veteran %>% as_ped(Surv(time, status) ~ ., max_time = 400)
```

```{r, pam-tve-karno, fig.width = 9, fig.height=3, warning=FALSE}
pam2 <- gam(ped_status ~ s(tend)+trt+prior+ celltype+s(age)+te(tend, karno) ,
  data = ped_vet, family = poisson(), offset = offset)
gg_tensor(pam2, ci=TRUE) + labs(x="time", y = "karno")
```


---
# TVE Karnofsky Score
```{r echo = TRUE, eval=FALSE, slices-karno-show}
p_tensor  <- gg_tensor(pam2)
p_slice_karno <- gg_slice(ped_vet, pam2, term="karno", tend=unique(tend),
  karno=c(40, 90), reference=list(karno=c(59)))
p_slice_time <- gg_slice(ped_vet, pam2, term="karno", tend=c(51, 300),
  karno = seq_range(karno, n=100), reference=list(karno=c(59))) +
  geom_hline(yintercept = 0, lty = 3)
# patchwork
p_tensor + p_slice_karno + p_slice_time
```

<a href="pammtools-slides-2018-06_files/figure-html/slices-karno-1.png">

```{r echo = FALSE, slices-karno, fig.width = 9, fig.height=3, warning=FALSE, dependson="p-tensor", dpi=1200}
p_tensor  <- gg_tensor(pam2) + geom_vline(xintercept = c(51, 300), lty = 2) +
  geom_hline(yintercept = c(40, 90), lty = 2) + ylab("karno") + xlab("time")
p_slice_karno <- gg_slice(ped_vet, pam2, term="karno", tend=unique(tend), karno=c(40, 90),
  reference=list(karno=c(59))) + geom_hline(yintercept = 0, lty = 3) +
  scale_color_brewer(palette = "Dark2") + scale_fill_brewer(palette = "Dark2")
p_slice_time <- gg_slice(ped_vet, pam2, term="karno", tend=c(51, 300), karno=seq_range(karno, n=100),
  reference=list(karno=c(59))) + geom_hline(yintercept = 0, lty = 3)
# patchwork
p_tensor + p_slice_karno + p_slice_time
```
</a>



---
# Tumor data: Data transformation
```{r tumor-data}
data(tumor)
# ?tumor
str(tumor)
## transform to PED format
ped_tumor <- as_ped(Surv(days, status)~., data = tumor, max_time = 3040)
```


---
# Tumor data: Model estimation
- All covariate effects are estimated as potentially time-varying


```{r pam-tumor}
pam_tumor_tve <- bam(
  formula = ped_status ~
    # baseline hazard
    ti(tend) +
    # complications
    complications + ti(tend, by = as.ordered(complications)) +
    # metastases
    metastases    + ti(tend, by = as.ordered(metastases))    +
    # sex
    sex           + ti(tend, by = as.ordered(sex))           +
    # transfusion
    transfusion   + ti(tend, by = as.ordered(transfusion))   +
    # resection
    resection     + ti(tend, by = as.ordered(resection))     +
    # charlson comorbidity score
    s(tend, by = charlson_score) +
    # age
    s(tend, by = age),
  data = ped_tumor, family = poisson(), offset = offset,
  method = "fREML", discrete = TRUE)
```

---
# Tumor data: Results
See `?get_cumu_coef` to obtain data used for visualization.

<a href="pammtools-slides-2018-06_files/figure-html/pam-tumor-viz-1.png">

```{r pam-tumor-viz, echo=FALSE, fig.width = 9, fig.height=6, warning=FALSE, message=FALSE, dpi = 1600, out.height="400px"}
library(timereg)
aalen_comp_metas <- aalen(
  formula  = Surv(days, status)~ age + sex + transfusion +
    complications + metastases + transfusion + charlson_score + resection,
  data     = tumor)

cumu_coef_df  <- get_cumu_coef(aalen_comp_metas,
  terms=c("age", "sex", "transfusion", "complications", "metastases",
    "charlson_score")) %>%
  mutate(
    variable = case_when(
      variable == "complicationsyes" ~ "complications (yes)",
      variable == "metastasesyes"    ~ "metastases (yes)",
      variable == "sexfemale"        ~ "sex (female)",
      variable == "transfusionyes"   ~ "transfusion (yes)",
      TRUE ~ as.character(variable))) %>%
  bind_rows(
    get_cumu_coef(pam_tumor_tve, ped_tumor,
      terms=c("age", "sex", "transfusion", "complications", "metastases",
        "charlson_score"), nsim=200L))
ggplot(cumu_coef_df, aes(x=time, y = cumu_hazard)) +
  geom_ribbon(aes(fill=method, ymin = cumu_lower, ymax=cumu_upper), alpha=0.3) +
  geom_line(aes(col=method), lwd=1.2) +  geom_hline(yintercept = 0, lty = 2) +
  scale_color_manual(values=c("grey50", Set1[2])) +
  scale_fill_manual(values=c("grey30", Set1[2])) +
  facet_wrap(~variable, scales="free_y") +
  ylab("Cumulative coefficient")
```
</a>


---
# Custom visualization

- <a href="https://adibender.github.io/pammtools/reference/newdata.html">`?make_newdata`</a>

  - creates data set with specified covariate values (functions can be used)
  - sets all other covariates to mean values/reference category

- `add_*` functions add quantity of interest, e.g.,
  - `add_term`: term contribution to the log-hazard
  - `add_hazard`/`add_cumu_hazard`: Hazard and cumulative hazard
  - `add_surv_prob`: Survival probabilities



```{r, make-newdf, fig.height=3, fig.width = 8, fig.show = "hide"}
# create newdata
new_df <- ped_tumor %>%
  make_newdata(tend=unique(tend), complications=unique(complications)) %>%
  group_by(complications) %>% #necessary for correct cumulative hazard and survival
  add_cumu_hazard(pam_tumor_tve) %>% add_surv_prob(pam_tumor_tve)
```


---
# Custom visualization
<a href = "pammtools-slides-2018-06_files/figure-html/viz-newdf-1.png">

```{r viz-newdf, fig.height=3, fig.width = 8}
# plot
p_cumu <- ggplot(new_df, aes(x = tend, y = cumu_hazard, fill = complications,
    ymin = cumu_lower, ymax = cumu_upper)) +
    geom_ribbon(alpha = 0.3) + geom_line(aes(col = complications))
p_surv <- p_cumu + aes(y = surv_prob, ymin = surv_lower, ymax = surv_upper)
p_cumu + p_surv
```

</a>


---
# Custom visualization

<a href = "pammtools-slides-2018-06_files/figure-html/ex-term-viz-1.png">

```{r, ex-term-viz, out.height="200px", fig.width=3, fig.height=3}
new_df2 <- ped_veteran %>%
  make_newdata(age = seq_range(age, n=20)) %>%
  add_term(pam2, term="*age")
new_df2 %>% slice(1:4) %>% select(celltype, age, fit:ci_upper)

ggplot(new_df2, aes(x = age, y = fit)) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha=0.2) + geom_line()
```
</a>



---
# Cumulative effects

- In general the partial effect can be a tri-variate function of
  - $\tz$ time at which exposure was observed
  - $z(\tz)$ the value of the exposure observed at time $\tz$
  - $t$ the follow-up time

- The cumulative effect is obtained by integrating over all $\tz$ within a
pre-specified time-window $\mathcal{T}(t)$

$$g(\bfz, t) = \int_{\mathcal{T}(t)} h(t, \tz, z(\tz))\mathrm{d}\tz \approx \sum_{\tz \in \mathcal{T}(t)} \Delta_{\tz}h(t,\tz,z(\tz))$$

<a href = "pammtools-slides-2018-06_files/figure-html/LL-ex-0-1.png">

```{r LL-ex-0, fig.show="hide"}
gg_laglead(0:10, tz=0:9, ll_fun = function(t, tz) {t >= tz})
```
</a>

---
# Cumulative effects
```{r ex-cumu-1, echo=FALSE}
# basic data
set.seed(7042018)
# create data set with covariates
n <-1000
df <- tibble::tibble(x1 = runif(n, -3, 3), x2 = runif(n, 0, 6))
# baseline hazard function
f0 <- function(t) {dgamma(t, 8, 2) * 6}
# simulate data from PEXP
# define follow-up time grid for simulation
# (arbitrary, but check that enough events are observed over follow-up)
time_grid <- seq(0, 10, by = 1)
# baseline hazard
f0 <- function(t) {dgamma(t, 8, 2) * 6}
# define time grid on which TDC is observed
# (arbitrary, but lag-lead matrix will depend on it)
tz <- seq(-5, 5, by = 1)
# define function that generates nz exposures z(t_{z,1}), ..., z(t_{z,Q})
rng_z = function(nz) {
as.numeric(arima.sim(n = nz, list(ar = c(.8, -.1))))
}
## add TDCs to data set
df <- df %>% add_tdc(tz, rng_z)
# define lag-lead function: integrate over the preceding 12 time units
ll_fun <- function(t, tz) {(t >= tz)}
# gg_laglead(0:10, -5:5, ll_fun)
# partial effect h(t - tz) * z
f_wce <- function(t, tz, z) {
  0.5 * (dnorm(t - tz, 6, 2.5)) * z
}
simdf_wce <- sim_pexp(
  formula = ~ -3.5 + f0(t) -0.5*x1 + sqrt(x2)|
    fcumu(t, tz, z.tz, f_xyz=f_wce, ll_fun=ll_fun),
  data = df, cut = time_grid)
```

- Consider simulated data were a covariate was observed at time points
$\tz \in \{-5,-4,\ldots, 0, \ldots, 4, 5\}$ (relative to the beginning of the
follow-up)

- The true hazard is given by
$\lambda(t; \bfz) = \lambda_0(t)\exp\left(\cdots + \sum_{\tz \leq t} h(t-\tz)z(\tz)\right)$


```{r wiz-sim_df, echo = FALSE, fig.width = 9, fig.height=3}
LL_df  <- get_laglead(0:10, -5:5, ll_fun)
LL_df <- LL_df %>%
  mutate(latency = t - tz,
    partial = f_wce(t, tz, z = 1)) %>%
  filter(t != 0)

gg_partial <- LL_df %>% filter(LL!=0) %>%
  ggplot(aes(x = t - tz, y = partial)) +
    geom_line() +
    ylab(expression(h(t - t[z]))) + xlab(expression(t - t[z]))

LL_df$partial[LL_df$LL == 0] <- NA
gg_partial_LL <- ggplot(LL_df,
    aes(x = t, y = tz, fill = partial * LL, z = partial * LL)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(name = expression(t[z]), expand = c(0, 0)) +
  geom_tile(colour = NA) +
  scale_fill_gradient2(name = expression(h(t - t[z])),
    na.value = "grey30", low = "steelblue", high = "firebrick2")

gg_cumeff <- LL_df %>%  group_by(t) %>%
  summarize(g_z = sum(partial*LL, na.rm=T)) %>%
  ggplot(aes(x = t, y = g_z)) +
    geom_line() +
    ylab(expression(g(bold(z) == 1, t))) +
    ylim(c(.2, .6))
gg_partial + gg_partial_LL + gg_cumeff
```

---
# Cumulative effects II
```{r}
head(simdf_wce)
simdf_wce %>% slice(1) %>% unnest() %>% slice(1:4)
```

---
# Cumulative effects III

To estimate a cumulative effect of `z.tz` (using `mgcv`), the data has to be
transformed into a special PED format with matrix columns


```{r}
ped <- as_ped(
  data = simdf_wce,
  formula = Surv(time, status) ~ x1 + x2| cumulative(latency(tz), z.tz, tz_var="tz",
    ll_fun = ll_fun),
  cut = seq(0, 10, by = 1))
str(ped, 1, give.attr=FALSE)
```

---
# Cumulative effects IV

```{r head-latency}
head(ped[ c("id", "interval", "tz_latency")])
```

---
# Cumulative effects V

```{r head-z}
head(ped[ c("id", "interval", "z.tz")])
```

---
# Cumulative effects VI

```{r head-LL}
head(ped[ c("id", "interval", "LL")])
```

<a href="pammtools-slides-2018-06_files/figure-html/ex-LL-1.png">

```{r ex-LL, fig.width = 9.4, fig.height=3, out.height="200px"}
gg_laglead(0:10, -5:5, ll_fun = function(t, tz) {t >= tz}) +
gg_laglead(0:10, -5:5, ll_fun = function(t, tz) t >= tz & t < tz + 3) +
gg_laglead(0:10, -5:5, ll_fun = function(t, tz) t == tz)
```
</a>




---
# Cumulative effects: Model estimation

.font80[
```{r}
mod_wce <- gam(
  formula = ped_status ~ s(tend) + s(x1) + s(x2) + s(tz_latency, by = z.tz * LL),
  data = ped, family = poisson(), offset = offset, method = "REML")
summary(mod_wce)
```
]

---
# Cumulative effects: Results
```{r, conv-funs-cumu, echo=TRUE, eval=FALSE}
gg_slice(ped, mod_wce, term = "z.tz", tz_latency = seq(0, 15, by = 1),
  z.tz = c(1), LL=c(1))
gg_partial_ll(ped, mod_wce, "z.tz", tend = seq(0, 10, by = 1),
  tz_latency = seq(0, 15, by = 1), z.tz=c(1))
gg_cumu_eff(ped, mod_wce, "z.tz", z1 = 1)
```

```{r, prod-viz-mod-wce, echo=FALSE, warning = FALSE, fig.width = 9, fig.height = 3}
p_slice <- gg_slice(ped, mod_wce, term = "z.tz",
    tz_latency = seq(0, 15, by = 1), z.tz = c(1), LL=c(1)) +
  scale_x_continuous(expand=c(0,0)) +
  geom_line(aes(x = tz_latency, y = f_wce(tend, tend-tz_latency, z = 1)), col = 2) +
  xlab(expression(t - t[z])) + ylab(expression(h(t - t[z]))) +
  geom_vline(xintercept = c(0, 10), lty = 2)
p_partial_ll <- gg_partial_ll(ped, mod_wce, "z.tz",
  tend = seq(0, 10, by = 1), tz_latency = seq(0, 15, by = 1), z.tz=c(1)) +
  geom_vline(xintercept=c(4, 5), lty = 1, lwd=1.2) +
  theme(legend.position = "right")
p_cumu_eff   <- gg_cumu_eff(ped, mod_wce, "z.tz", z1 = 1) +
  geom_vline(xintercept = 5, lty = 3) +
  geom_point(aes(y=cumu_eff)) + ylab(expression(g(bold(z), t)))
cumu_df_wce <- crossing(tend=time_grid, tz=tz, z.tz=1) %>%
  mutate(
    cumu_wce  = f_wce(tend, tz, z.tz)*ll_fun(tend, tz)) %>%
  group_by(tend) %>%
  summarize(
    cumu_wce  = sum(1 * cumu_wce)) # pre-multiply with quadrature weights

p_slice + p_partial_ll +
  p_cumu_eff + geom_line(data=cumu_df_wce, aes(y=cumu_wce), col=2)
```


$\ra$ [More examples (simulated data)](https://arxiv.org/pdf/1806.01042) <br>
$\ra$ [Example nutrition in the ICU](https://adibender.netlify.com/slides/london12_2017/ercim-pamm#21)

---
# Summary

- PAMMs offer a powerful framework for time-to-event data analysis, including
  - multiple, smooth time-varying-effects
  - time-dependent covariates
  - cumulative effects of time-dependent covariates
  - [random effects](https://adibender.github.io/pammtools/articles/frailty.html)

- **`pammtools`** offers utility functions that handle
  - preprocessing: `as_ped`
  - post-processing: `make_newdata`, `add_*`
  - visualization: `gg_slice`, `gg_smooth`, `gg_tensor`, `gg_partial_ll`, ...


- help/feedback welcomed at https://github.com/adibender/pammtools/issues


---
# Links and Acknowledgments
- For more details on PAMMs/**`pammtools`**:
  - Bender, Andreas, and Fabian Scheipl. “Pammtools: Piece-Wise Exponential Additive Mixed Modeling Tools.” ArXiv:1806.01042 [Stat], June 4, 2018. http://arxiv.org/abs/1806.01042.
  - Bender, Andreas, Andreas Groll, and Fabian Scheipl. “A Generalized Additive Model Approach to Time-to-Event Analysis.” Statistical Modelling, February 14, 2018, 1471082X17748083. https://doi.org/10.1177/1471082X17748083.
  - Bender, Andreas, Fabian Scheipl, Wolfgang Hartl, Andrew G. Day, and Helmut Küchenhoff. “Penalized Estimation of Complex, Non-Linear Exposure-Lag-Response Associations.” Biostatistics, December 2, 2018. https://doi.org/10.1093/biostatistics/kxy003.

- Slides created via [Yihui Xie](https://twitter.com/xieyihui)'s R package [**xaringan**](https://github.com/yihui/xaringan)
with (modified) [Metropolis theme](https://slides.yihui.name/xaringan/#34)

- All graphics have been created using Hadley Whickham's [ggplot2](http://ggplot2.tidyverse.org/)

- Models are estimated using Simon Wood's [mgcv](https://cran.r-project.org/web/packages/mgcv/index.html)
`r Citep(bib, "Wood2011", "Wood2017")`

- Links:
<a itemprop="sameAs" href="https://adibender.netlify.com/">
<span class="fa fa-globe"></span>
</a>
<a itemprop="sameAs" href ="https://github.com/adibender"><span class="fa
 fa-github fa-lg fa-fw"></span>
</a>
<a itemprop="sameAs" rel="noopener noreferrer" href="https://www.researchgate.net/profile/Andreas_Bender4" target="_blank"> <i class="ai ai-researchgate big-icon"></i> </a>
<a itemprop="sameAs" href="//twitter.com/adiBender" target="_blank">
<i class="fa fa-twitter big-icon"></i> </a>


---
# References
```{r, results = "asis", echo=FALSE, eval=TRUE}
PrintBibliography(bib)
```
